# ğŸ’¬ Local RAG Chatbot (Ollama + LangChain + FAISS)

A fully local Retrieval-Augmented Generation (RAG) chatbot built using LangChain, Ollama (Mistral), FAISS vector database, and Streamlit.

This project demonstrates end-to-end LLM application development including embeddings, vector search, conversational memory, and local model inference â€” all running without paid APIs.

---

## ğŸš€ Features

- ğŸ”¹ Local LLM using Ollama (Mistral)
- ğŸ”¹ Retrieval-Augmented Generation (RAG)
- ğŸ”¹ FAISS vector database
- ğŸ”¹ HuggingFace sentence-transformer embeddings
- ğŸ”¹ Conversational memory
- ğŸ”¹ Streamlit chat interface
- ğŸ”¹ Fully offline (no OpenAI API required)

---

## ğŸ—ï¸ Tech Stack

- Python 3.10
- LangChain (v0.1.20)
- Ollama (Mistral model)
- FAISS
- HuggingFace Embeddings
- Streamlit
- PyTorch (CPU)

---

## ğŸ“‚ Project Structure

chatbot/
â”‚
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ chain.py # RAG pipeline
â”œâ”€â”€ ingest.py # Creates FAISS index
â”œâ”€â”€ data.txt # Knowledge base
â”œâ”€â”€ faiss_index/ # Generated vector store
â””â”€â”€ README.md


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd chatbot
2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate
3ï¸âƒ£ Install Dependencies
pip install langchain==0.1.20
pip install langchain-community==0.0.38
pip install langchain-huggingface==0.0.3
pip install langsmith==0.1.147
pip install faiss-cpu
pip install sentence-transformers
pip install streamlit
pip install python-dotenv
pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cpu
pip install numpy==1.26.4
4ï¸âƒ£ Install Ollama & Pull Model
Download Ollama from:

https://ollama.com

Then:

ollama pull mistral
5ï¸âƒ£ Create Vector Index
python ingest.py
6ï¸âƒ£ Run Application
python -m streamlit run app.py
Open:

http://localhost:8501
ğŸ§  How It Works
data.txt is split into chunks.

Chunks are converted into embeddings.

FAISS stores embeddings locally.

On user query:

Relevant chunks are retrieved.

Sent to Mistral via Ollama.

Response generated with conversational memory.

ğŸ“¸ Example Use Cases
Resume chatbot

Personal knowledge assistant

Documentation search

Offline AI assistant

Learning RAG architecture

ğŸ¯ Key Learning Outcomes
Built full RAG pipeline from scratch

Integrated local LLM (no external APIs)

Handled dependency & version conflicts

Implemented vector search with FAISS

Designed conversational memory system

ğŸ“Œ Future Improvements
Add streaming responses

Add source citation display

Dockerize application

Deploy via FastAPI backend

Add authentication
